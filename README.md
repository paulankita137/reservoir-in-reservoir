Reservoirs-in-Reservoir (IJCNN 2024)

This is a work-in-progress repository. The repository implements the Reservoirs-in-Reservoir learning architecture of Recurrently connected Spiking Neural Networks. The learning described in the paper is applied to a set of aperiodic functions. The train and test files are for training the networks and testing the networks respectively. The number of neurons, number of trials can be altered through the parameters in the respective train and test files to experiment with. The rate based and time-to-first-spike based implementations are available in separate folders. For any questions send an email to - ankita.paul@drexel.edu

The proposed training procedure consists of generating targets for both the recurrently-connected hidden layer and the output layer (i.e., for a full RSNN system) in a reservoir pool consisting of small reservoirs, and using the recursive least square-based full First-Order and Reduced Control Error (FORCE) algorithm to fit the activity of each layer to its target. We demonstrate the proposed R-i-R training procedure to model 8 dynamic systems using RSNNs with leaky integrate and fire (LIF) neurons and spike rate-based encoding. For energy- efficient hardware implementation, an alternative time-to-first-spike (TTFS) encoding is implemented for the full-FORCE train-ing procedure. 

This repository is being built and will be available shortly. 
